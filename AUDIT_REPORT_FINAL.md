# é¡¹ç›®ä»£ç å…¨é¢å®¡è®¡æŠ¥å‘Š

**é¡¹ç›®**ï¼šDeepFilterNet éŸ³è‰²ä¿®å¤æ¨¡å‹è®­ç»ƒç³»ç»Ÿ  
**å®¡è®¡æ—¥æœŸ**ï¼š2024å¹´  
**å®¡è®¡èŒƒå›´**ï¼šæ•°æ®é›†å‡†ç¡®æ€§ã€è®­ç»ƒæ•ˆç‡ã€è®­ç»ƒæ•ˆæœ  
**å®¡è®¡ç‰ˆæœ¬**ï¼šFinal v1.0

---

## ç›®å½•

1. [æ•°æ®é›†å‡†ç¡®æ€§é—®é¢˜](#ä¸€æ•°æ®é›†å‡†ç¡®æ€§é—®é¢˜)
2. [è®­ç»ƒæ•ˆç‡é—®é¢˜](#äºŒè®­ç»ƒæ•ˆç‡é—®é¢˜)
3. [è®­ç»ƒæ•ˆæœé—®é¢˜](#ä¸‰è®­ç»ƒæ•ˆæœé—®é¢˜)
4. [ä»£ç é€»è¾‘é”™è¯¯](#å››ä»£ç é€»è¾‘é”™è¯¯)
5. [å®ç°ç»†èŠ‚é—®é¢˜](#äº”å®ç°ç»†èŠ‚é—®é¢˜)
6. [ä¿®å¤ä¼˜å…ˆçº§](#å…­ä¿®å¤ä¼˜å…ˆçº§)
7. [æ€»ä½“è¯„ä»·](#ä¸ƒæ€»ä½“è¯„ä»·)

---

## ä¸€ã€æ•°æ®é›†å‡†ç¡®æ€§é—®é¢˜

### ğŸ”´ P0-1ï¼šæ•°æ®å¯¹é½åéšæœºè£å‰ªç ´åå¯¹é½å…³ç³»ï¼ˆä¸¥é‡ï¼‰

**ä½ç½®**ï¼š`data/dataset.py` ç¬¬ 145-160 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
# å¯¹é½ DF æ½œåœ¨å»¶è¿Ÿï¼ˆé€æ ·æœ¬ï¼‰
if self.align_df_delay and self.align_max_shift > 0 and min_len > 1000:
    offset = self._estimate_offset(clean, degraded, self.align_max_shift, self.sample_rate)
    if offset != 0:
        degraded, clean = self._apply_offset(degraded, clean, offset)

# éšæœºè£å‰ªæˆ–å¡«å……ï¼ˆå¯¹é½åé•¿åº¦å¯èƒ½ç•¥çŸ­ï¼‰
current_len = len(degraded)
if current_len >= self.segment_length:
    if self.augment:
        start = random.randint(0, current_len - self.segment_length)  # âŒ ç ´åå¯¹é½
    degraded = degraded[start:start + self.segment_length]
    clean = clean[start:start + self.segment_length]
```

**é—®é¢˜åˆ†æ**ï¼š
- å¯¹é½çš„ç›®çš„æ˜¯è¡¥å¿ DF å¼•å…¥çš„å›ºå®šå»¶è¿Ÿï¼ˆå¦‚ degraded æ¯” clean æ»å N ä¸ªæ ·æœ¬ï¼‰
- å¯¹é½åä¸¤ä¸ªä¿¡å·å·²ç»åœ¨æ—¶é—´è½´ä¸ŠåŒ¹é…
- éšæœºè£å‰ªä½¿ç”¨ç›¸åŒçš„ `start` ç´¢å¼•ï¼Œçœ‹ä¼¼æ²¡é—®é¢˜
- **ä½†é—®é¢˜æ˜¯**ï¼šå¯¹é½æ“ä½œé€šè¿‡è£æ‰å¤´éƒ¨æˆ–å°¾éƒ¨æ¥å®ç°ï¼Œéšæœºè£å‰ªåï¼Œä¸¤ä¸ªä¿¡å·åœ¨æŸäº›ä½ç½®å¯èƒ½ä¸å†å®Œå…¨å¯¹é½

**å½±å“**ï¼š
- **ä¸¥é‡å½±å“è®­ç»ƒè´¨é‡**ï¼šæ¨¡å‹å­¦ä¹ çš„æ˜¯é”™ä½çš„ä¿¡å·å¯¹
- å¯¼è‡´ L1/STFT Loss æ— æ³•ä¸‹é™åˆ°ç†è®ºæœ€ä¼˜å€¼
- æ¨¡å‹å¯èƒ½å­¦ä¹ åˆ°é”™è¯¯çš„æ˜ å°„å…³ç³»

**ä¿®å¤å»ºè®®**ï¼š
```python
# æ–¹æ¡ˆ 1ï¼šå¯¹é½åä½¿ç”¨å›ºå®šè£å‰ªï¼ˆæ¨èï¼‰
if current_len >= self.segment_length:
    # å¯¹é½åå§‹ç»ˆä»å¼€å¤´è£å‰ªï¼Œä¿è¯å¯¹é½å…³ç³»
    start = 0  
    degraded = degraded[start:start + self.segment_length]
    clean = clean[start:start + self.segment_length]

# æ–¹æ¡ˆ 2ï¼šåœ¨å¯¹é½å‰è¿›è¡Œéšæœºè£å‰ªï¼ˆéœ€è¦å¤§é‡ä¿®æ”¹ï¼‰
# å…ˆéšæœºè£å‰ªï¼Œå†è¿›è¡Œå¯¹é½ä¼°è®¡
```

**ä¼˜å…ˆçº§**ï¼šğŸ”´ **P0 - å¿…é¡»ç«‹å³ä¿®å¤**

---

### ğŸ”´ P0-2ï¼šæ•°æ®å‡†å¤‡é˜¶æ®µ clean å’Œ degraded ä¸å¯¹åº”ï¼ˆä¸¥é‡ï¼‰

**ä½ç½®**ï¼š`data/prepare_dataset.py` ç¬¬ 216-255 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
# å½’ä¸€åŒ–
clean_seg = clean_seg / (np.abs(clean_seg).max() + 1e-8) * 0.9

# åˆ›å»ºé€€åŒ–ç‰ˆæœ¬
degraded = clean_seg.copy()

# éšæœºæ·»åŠ æ··å“ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
add_ir = random.random() < ir_prob and len(irs) > 0
if add_ir:
    ir_path = random.choice(irs)
    ir = _get_cached(...)
    degraded = add_reverb(degraded, ir)

# éšæœºæ·»åŠ å™ªå£°ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
if random.random() < noise_prob:
    snr = random.uniform(snr_range[0], snr_range[1])
    noise_arr = _get_cached(...)
    degraded = add_noise(degraded, noise_arr, snr)

# ä¿å­˜ï¼ˆè¿™é‡Œä¿å­˜çš„æ˜¯åŠ å™ªç‰ˆæœ¬ï¼Œåç»­æ‰¹é‡è¿‡ DFï¼‰
sf.write(clean_out, clean_seg, target_sr)  # âŒ ä¿å­˜çš„æ˜¯å½’ä¸€åŒ–åçš„ clean
sf.write(degraded_out, degraded, target_sr)  # ä¿å­˜çš„æ˜¯åŠ å™ª+æ··å“ç‰ˆæœ¬
```

**é—®é¢˜åˆ†æ**ï¼š
- `clean_out` ä¿å­˜çš„æ˜¯å½’ä¸€åŒ–åçš„å¹²å‡€è¯­éŸ³
- `degraded_out` ä¿å­˜çš„æ˜¯åŠ å™ª+æ··å“ç‰ˆæœ¬ï¼ˆæœªè¿‡ DFï¼‰
- åç»­é€šè¿‡ DeepFilterNet å¤„ç† `degraded_out`ï¼Œä½† **DF çš„è¾“å‡ºä¼šä¸åŸå§‹ clean äº§ç”Ÿå»¶è¿Ÿ/å¤±çœŸ**
- **å…³é”®é—®é¢˜**ï¼šä»£ç æ³¨é‡Šè¯´"åç»­æ‰¹é‡è¿‡ DF"ï¼Œä½† DF å¤„ç†åçš„è¾“å‡ºä¼šè¦†ç›– `degraded_out`
- **æ•°æ®æµä¸å¯¹**ï¼šåº”è¯¥æ˜¯ `DF(åŠ å™ªéŸ³é¢‘) vs åŸå§‹å¹²å‡€éŸ³é¢‘`ï¼Œè€Œä¸æ˜¯ `åŠ å™ªéŸ³é¢‘ vs åŸå§‹å¹²å‡€éŸ³é¢‘`

**å½±å“**ï¼š
- **ä¸¥é‡å½±å“æ•°æ®é›†å‡†ç¡®æ€§**
- è®­ç»ƒæ—¶è¾“å…¥æ˜¯ DF å¤„ç†åçš„éŸ³é¢‘ï¼Œç›®æ ‡æ˜¯åŸå§‹å¹²å‡€éŸ³é¢‘
- ä½†æ•°æ®å‡†å¤‡é˜¶æ®µä¿å­˜çš„ degraded æ˜¯åŠ å™ªç‰ˆæœ¬ï¼ˆæœªè¿‡ DFï¼‰ï¼Œåç»­æ‰é€šè¿‡ DF
- å¦‚æœæµç¨‹æœ‰è¯¯ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒé›†ä¸æ­£ç¡®

**éªŒè¯å»ºè®®**ï¼š
```python
# æ£€æŸ¥ run_deepfilter_batch æ˜¯å¦æ­£ç¡®æ›´æ–°äº†æ–‡ä»¶
# ç¬¬ 272-305 è¡Œï¼Œç¡®è®¤ DF å¤„ç†åè¾“å‡ºåˆ° degraded_dir
```

**ç»“è®º**ï¼š
- ä»£ç é€»è¾‘çœ‹èµ·æ¥æ˜¯ï¼š`noisy_dir`ï¼ˆåŠ å™ªï¼‰â†’ DF å¤„ç† â†’ `degraded_dir`ï¼ˆDF è¾“å‡ºï¼‰
- è®­ç»ƒæ—¶ä½¿ç”¨ `degraded_dir` vs `clean_dir`ï¼Œ**é€»è¾‘æ­£ç¡®**
- ä½†éœ€è¦ç¡®è®¤ `run_deepfilter_batch` ç¡®å®æ›´æ–°äº†æ‰€æœ‰æ–‡ä»¶

**ä¼˜å…ˆçº§**ï¼šğŸ”´ **P0 - éœ€è¦éªŒè¯å’Œç¡®è®¤**

---

### ğŸŸ¡ P1-1ï¼šæ··å“æ·»åŠ åèƒ½é‡å½’ä¸€åŒ–å¯èƒ½ç ´å SNR

**ä½ç½®**ï¼š`data/prepare_dataset.py` ç¬¬ 125-136 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
def add_reverb(audio: np.ndarray, ir: np.ndarray) -> np.ndarray:
    """æ·»åŠ æ··å“ï¼ˆå·ç§¯ï¼‰ï¼Œä¿æŒèƒ½é‡"""
    reverbed = signal.fftconvolve(audio, ir, mode='full')[:len(audio)]
    # èƒ½é‡å½’ä¸€åŒ–åˆ°åŸå§‹ RMS
    orig_rms = np.sqrt(np.mean(audio ** 2) + 1e-8)
    rev_rms = np.sqrt(np.mean(reverbed ** 2) + 1e-8)
    reverbed = reverbed * (orig_rms / rev_rms)  # âŒ æ”¹å˜äº†èƒ½é‡
    # å³°å€¼ä¿æŠ¤
    max_val = np.abs(reverbed).max()
    if max_val > 0.99:
        reverbed = reverbed * 0.99 / max_val  # âŒ å†æ¬¡æ”¹å˜èƒ½é‡
    return reverbed.astype(np.float32)
```

**é—®é¢˜åˆ†æ**ï¼š
- æ··å“åè¿›è¡Œèƒ½é‡å½’ä¸€åŒ–ï¼Œä½¿å¾— RMS ä¸åŸå§‹éŸ³é¢‘ç›¸åŒ
- éšåæ·»åŠ å™ªå£°æ—¶ï¼Œè®¡ç®—çš„ SNR æ˜¯åŸºäºå½’ä¸€åŒ–åçš„éŸ³é¢‘
- **ä½†å®é™…åœºæ™¯ä¸­**ï¼šæ··å“é€šå¸¸ä¼šæ”¹å˜éŸ³é¢‘èƒ½é‡ï¼Œä¸åº”è¯¥å¼ºåˆ¶å½’ä¸€åŒ–
- å½’ä¸€åŒ–åå†åŠ å™ªå£°ï¼Œå®é™… SNR ä¸é¢„æœŸä¸ç¬¦

**å½±å“**ï¼š
- æ•°æ®é›†ä¸­çš„ SNR ä¸å‡†ç¡®
- æ··å“åçš„éŸ³é¢‘èƒ½é‡è¢«äººä¸ºè°ƒæ•´ï¼Œä¸ç¬¦åˆçœŸå®åœºæ™¯

**ä¿®å¤å»ºè®®**ï¼š
```python
def add_reverb(audio: np.ndarray, ir: np.ndarray) -> np.ndarray:
    """æ·»åŠ æ··å“ï¼ˆå·ç§¯ï¼‰"""
    reverbed = signal.fftconvolve(audio, ir, mode='full')[:len(audio)]
    # åªåšå³°å€¼ä¿æŠ¤ï¼Œä¸åšèƒ½é‡å½’ä¸€åŒ–
    max_val = np.abs(reverbed).max()
    if max_val > 0.99:
        reverbed = reverbed * 0.99 / max_val
    return reverbed.astype(np.float32)
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®ä¿®å¤**

---

### ğŸŸ¡ P1-2ï¼šå™ªå£°æ·»åŠ åå†æ¬¡å½’ä¸€åŒ–ç ´å SNR

**ä½ç½®**ï¼š`data/prepare_dataset.py` ç¬¬ 139-167 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
def add_noise(audio: np.ndarray, noise: np.ndarray, snr_db: float) -> np.ndarray:
    """æŒ‰æŒ‡å®š SNR æ·»åŠ å™ªå£°"""
    # ... è®¡ç®— SNR ...
    noisy = audio + noise_gain * noise_segment
    
    # å½’ä¸€åŒ–é˜²æ­¢å‰Šæ³¢
    max_val = np.abs(noisy).max()
    if max_val > 0.99:
        noisy = noisy * 0.99 / max_val  # âŒ ç ´åäº† SNR
    
    return noisy.astype(np.float32)
```

**é—®é¢˜åˆ†æ**ï¼š
- æŒ‰æŒ‡å®š SNR æ·»åŠ å™ªå£°åï¼Œè¿›è¡Œå³°å€¼å½’ä¸€åŒ–
- å½’ä¸€åŒ–ä¼šæ”¹å˜éŸ³é¢‘å’Œå™ªå£°çš„ç›¸å¯¹åŠŸç‡ï¼Œ**ç ´åäº† SNR**
- å®é™… SNR ä¼šæ¯”é¢„æœŸæ›´é«˜ï¼ˆå› ä¸ºæ•´ä½“è¢«å‹ç¼©ï¼‰

**å½±å“**ï¼š
- æ•°æ®é›†ä¸­çš„ SNR æ ‡ç­¾ä¸å‡†ç¡®
- ä½ SNR æ ·æœ¬ï¼ˆ5-10 dBï¼‰å¯èƒ½å®é™…ä¸Šæ˜¯ä¸­ç­‰ SNRï¼ˆ8-13 dBï¼‰

**ä¿®å¤å»ºè®®**ï¼š
```python
def add_noise(audio: np.ndarray, noise: np.ndarray, snr_db: float) -> np.ndarray:
    """æŒ‰æŒ‡å®š SNR æ·»åŠ å™ªå£°"""
    # ... è®¡ç®— SNR ...
    noisy = audio + noise_gain * noise_segment
    
    # æ–¹æ¡ˆ 1ï¼šè½¯å‰Šæ³¢è€Œéå½’ä¸€åŒ–
    noisy = np.tanh(noisy * 0.9)  # è½¯å‰Šæ³¢ï¼Œä¿æŒç›¸å¯¹å…³ç³»
    
    # æ–¹æ¡ˆ 2ï¼šä¸åšå¤„ç†ï¼Œå…è®¸å‰Šæ³¢ï¼ˆåç»­ DF ä¼šå¤„ç†ï¼‰
    # noisy = np.clip(noisy, -1.0, 1.0)
    
    return noisy.astype(np.float32)
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®ä¿®å¤**

---

### ğŸŸ¡ P1-3ï¼šæ•°æ®å¢å¼ºå¢ç›ŠåŒæ—¶åº”ç”¨äº degraded å’Œ clean

**ä½ç½®**ï¼š`data/dataset.py` ç¬¬ 175-179 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
if self.augment:
    # æ¸©å’Œå¢ç›Šï¼ˆçº¦ Â±1dBï¼‰
    gain = random.uniform(0.9, 1.1)
    degraded = degraded * gain  # âŒ åŒæ—¶å¢å¼º
    clean = clean * gain         # âŒ åŒæ—¶å¢å¼º
```

**é—®é¢˜åˆ†æ**ï¼š
- åŒæ—¶å¯¹ degraded å’Œ clean åº”ç”¨ç›¸åŒå¢ç›Š
- ç›¸å¯¹å…³ç³»ä¸å˜ï¼Œ**æ•°æ®å¢å¼ºæ•ˆæœæœ‰é™**
- åªæ˜¯çº¿æ€§ç¼©æ”¾ï¼Œæ¨¡å‹å¾ˆå®¹æ˜“å­¦ä¼š

**å½±å“**ï¼š
- æ•°æ®å¢å¼ºæ„ä¹‰ä¸å¤§
- ä¸å¦‚ä¸åšæˆ–æ”¹æˆæ›´æœ‰æ„ä¹‰çš„å¢å¼º

**ä¿®å¤å»ºè®®**ï¼š
```python
# æ–¹æ¡ˆ 1ï¼šåªå¢å¼º degradedï¼ˆæ¨èï¼‰
if self.augment:
    gain = random.uniform(0.9, 1.1)
    degraded = degraded * gain
    # clean ä¸å˜

# æ–¹æ¡ˆ 2ï¼šç§»é™¤è¯¥å¢å¼º
# æ³¨é‡Šæ‰å¢ç›Šå¢å¼ºä»£ç 
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®ä¿®å¤**

---

### ğŸŸ¢ P2-1ï¼šå¯¹é½ä¼°è®¡ç®—æ³•å¯èƒ½ä¸å¤Ÿé²æ£’

**ä½ç½®**ï¼š`data/dataset.py` ç¬¬ 85-116 è¡Œ

**é—®é¢˜æè¿°**ï¼š
- ä½¿ç”¨ç®€å•çš„ç‚¹ç§¯ç›¸å…³æ€§ä¼°è®¡åç§»
- å¯¹äºå™ªå£°è¾ƒå¤§çš„ä¿¡å·å¯èƒ½ä¸å‡†ç¡®
- æ¯ä¸ªæ ·æœ¬éƒ½è®¡ç®—ï¼Œå¼€é”€è¾ƒå¤§

**å½±å“**ï¼šå¯¹é½ä¸å‡†ç¡®å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

## äºŒã€è®­ç»ƒæ•ˆç‡é—®é¢˜

### ğŸ”´ P0-3ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨å˜é‡ä½œç”¨åŸŸé”™è¯¯ï¼ˆä¸¥é‡ï¼‰

**ä½ç½®**ï¼š`train.py` ç¬¬ 544-573 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
for batch_idx, (degraded, clean) in enumerate(pbar):
    losses = self.train_step(degraded, clean)
    self.global_step += 1
    # ... æ—¥å¿—è®°å½• ...

# CosineAnnealingWarmRestarts æŒ‰ step æ›´æ–°
if not self.scheduler_step_per_epoch and num_batches > 0:  # âŒ åœ¨å¾ªç¯å¤–
    if self.enable_scheduler:
        step_frac = epoch + batch_idx / num_batches  # âŒ batch_idx æ˜¯å¾ªç¯ç»“æŸåçš„å€¼
        self.scheduler_g.step(step_frac)
        self.scheduler_d.step(step_frac)
```

**é—®é¢˜åˆ†æ**ï¼š
- `batch_idx` åœ¨ for å¾ªç¯å†…å®šä¹‰
- è°ƒåº¦å™¨æ›´æ–°åœ¨å¾ªç¯å¤–æ‰§è¡Œ
- **æ¯ä¸ª epoch åªæ›´æ–°ä¸€æ¬¡å­¦ä¹ ç‡**ï¼Œè€Œéæ¯ä¸ª step æ›´æ–°ä¸€æ¬¡
- `batch_idx` æ˜¯æœ€åä¸€ä¸ªå€¼ï¼ˆ`num_batches - 1`ï¼‰ï¼Œå¯¼è‡´å­¦ä¹ ç‡ä¸å‡†ç¡®

**å½±å“**ï¼š
- **ä¸¥é‡å½±å“è®­ç»ƒæ•ˆæœ**
- CosineAnnealingWarmRestarts çš„ warm restarts æœºåˆ¶å¤±æ•ˆ
- å­¦ä¹ ç‡è°ƒåº¦ä¸å‡†ç¡®ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šæˆ–æ”¶æ•›æ…¢

**ä¿®å¤å»ºè®®**ï¼š
```python
for batch_idx, (degraded, clean) in enumerate(pbar):
    losses = self.train_step(degraded, clean)
    self.global_step += 1
    
    # ... æ—¥å¿—è®°å½• ...
    
    # CosineAnnealingWarmRestarts æŒ‰ step æ›´æ–°ï¼ˆç§»åˆ°å¾ªç¯å†…ï¼‰
    if not self.scheduler_step_per_epoch and num_batches > 0:
        if self.enable_scheduler:
            step_frac = epoch + batch_idx / num_batches
            self.scheduler_g.step(step_frac)
            self.scheduler_d.step(step_frac)

# CosineAnnealingLR æŒ‰ epoch æ›´æ–°ï¼ˆä¿æŒåœ¨å¾ªç¯å¤–ï¼‰
if self.scheduler_step_per_epoch:
    if self.enable_scheduler:
        self.scheduler_g.step()
        self.scheduler_d.step()
```

**ä¼˜å…ˆçº§**ï¼šğŸ”´ **P0 - å¿…é¡»ç«‹å³ä¿®å¤**

---

### ğŸ”´ P0-4ï¼šnum_workers=0 ä¸¥é‡é™åˆ¶æ•°æ®åŠ è½½é€Ÿåº¦ï¼ˆä¸¥é‡ï¼‰

**ä½ç½®**ï¼š`configs/default.yaml` ç¬¬ 47 è¡Œï¼Œ`train.py` ç¬¬ 282-299 è¡Œ

**é—®é¢˜é…ç½®**ï¼š
```yaml
data:
  num_workers: 0  # âŒ å•è¿›ç¨‹åŠ è½½
```

**é—®é¢˜ä»£ç **ï¼š
```python
self.train_loader = DataLoader(
    train_dataset,
    batch_size=train_config['batch_size'],
    shuffle=(train_sampler is None),
    sampler=train_sampler,
    num_workers=data_config['num_workers'],  # 0
    pin_memory=False,  # âŒ æœªå¯ç”¨
    persistent_workers=False,  # å› ä¸º num_workers=0
    prefetch_factor=1,
    drop_last=True,
)
```

**é—®é¢˜åˆ†æ**ï¼š
- `num_workers=0` è¡¨ç¤ºåœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ•°æ®ï¼Œ**å®Œå…¨æ²¡æœ‰å¹¶è¡Œ**
- é…ç½®æ³¨é‡Šè¯´"é¿å… shm/æ–‡ä»¶ç³»ç»Ÿå…±äº«é—®é¢˜"ï¼Œä½†è¿™ä¼šä¸¥é‡å½±å“è®­ç»ƒé€Ÿåº¦
- `pin_memory=False` è¿›ä¸€æ­¥é™ä½æ•°æ®ä¼ è¾“æ•ˆç‡
- æ¯ä¸ª batch çš„åŠ è½½æ—¶é—´åŒ…æ‹¬ï¼š
  - è¯»å–éŸ³é¢‘æ–‡ä»¶
  - å¯¹é½è®¡ç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
  - æ•°æ®å¢å¼º
  - è½¬æ¢ä¸º Tensor
- **GPU ä¼šç­‰å¾…æ•°æ®åŠ è½½**ï¼Œåˆ©ç”¨ç‡ä½

**å½±å“**ï¼š
- **ä¸¥é‡å½±å“è®­ç»ƒæ•ˆç‡**
- GPU åˆ©ç”¨ç‡ä½ï¼Œå¤§é‡æ—¶é—´æµªè´¹åœ¨æ•°æ®åŠ è½½
- è®­ç»ƒæ—¶é—´å¯èƒ½å¢åŠ  2-5 å€

**ä¿®å¤å»ºè®®**ï¼š
```yaml
# configs/default.yaml
data:
  num_workers: 4  # æˆ–æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼ˆå»ºè®® 2-8ï¼‰
```

```python
# train.py
self.train_loader = DataLoader(
    train_dataset,
    batch_size=train_config['batch_size'],
    shuffle=(train_sampler is None),
    sampler=train_sampler,
    num_workers=4,  # æ”¹ä¸ºå¤šè¿›ç¨‹
    pin_memory=True if self.device.type == 'cuda' else False,  # GPU æ—¶å¯ç”¨
    persistent_workers=True if num_workers > 0 else False,  # ä¿æŒ worker è¿›ç¨‹
    prefetch_factor=2,  # å¢åŠ é¢„å–
    drop_last=True,
)
```

**ä¼˜å…ˆçº§**ï¼šğŸ”´ **P0 - å¿…é¡»ç«‹å³ä¿®å¤**

---

### ğŸŸ¡ P1-4ï¼šéªŒè¯é›† drop_last=True æµªè´¹æ•°æ®

**ä½ç½®**ï¼š`train.py` ç¬¬ 297 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
self.val_loader = DataLoader(
    val_dataset,
    batch_size=train_config['batch_size'],
    shuffle=False,
    sampler=None,
    num_workers=data_config['num_workers'],
    pin_memory=False,
    persistent_workers=False,
    prefetch_factor=1,
    drop_last=True,  # âŒ éªŒè¯é›†ä¹Ÿä¸¢å¼ƒæœ€åä¸€ä¸ª batch
)
```

**å½±å“**ï¼šéªŒè¯é›†æ•°æ®åˆ©ç”¨ä¸å……åˆ†ï¼ŒéªŒè¯æŒ‡æ ‡ç•¥æœ‰åå·®

**ä¿®å¤å»ºè®®**ï¼š`drop_last=False`

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®ä¿®å¤**

---

### ğŸŸ¡ P1-5ï¼šGradScaler æœªæŒ‡å®š device å‚æ•°

**ä½ç½®**ï¼š`train.py` ç¬¬ 96-97 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
self.scaler_g = GradScaler()  # âŒ æœªæŒ‡å®š device
self.scaler_d = GradScaler()  # âŒ æœªæŒ‡å®š device
```

**é—®é¢˜åˆ†æ**ï¼š
- PyTorch 2.0+ çš„ `GradScaler` éœ€è¦æŒ‡å®š `device` å‚æ•°
- æœªæŒ‡å®šæ—¶é»˜è®¤ä¸º CUDAï¼Œåœ¨ CPU è®­ç»ƒæ—¶å¯èƒ½å‡ºé”™
- è™½ç„¶ä»£ç åœ¨ `autocast` ä¸­æŒ‡å®šäº† `device_type`ï¼Œä½† `GradScaler` ä¹Ÿåº”è¯¥åŒ¹é…

**ä¿®å¤å»ºè®®**ï¼š
```python
self.scaler_g = GradScaler(device=self.device.type)
self.scaler_d = GradScaler(device=self.device.type)
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®ä¿®å¤**

---

### ğŸŸ¢ P2-2ï¼šæ•°æ®å¯¹é½è®¡ç®—å¼€é”€å¤§

**ä½ç½®**ï¼š`data/dataset.py` ç¬¬ 85-116 è¡Œ

**é—®é¢˜æè¿°**ï¼š
- æ¯ä¸ªæ ·æœ¬éƒ½è®¡ç®—å¯¹é½åç§»ï¼Œæ—¶é—´å¤æ‚åº¦ O(max_shift)
- `max_shift=2000` æ—¶ï¼Œæ¯ä¸ªæ ·æœ¬éœ€è¦éå† 4000+ ä¸ª lag

**å½±å“**ï¼šæ•°æ®åŠ è½½å¯èƒ½æˆä¸ºç“¶é¢ˆï¼ˆç‰¹åˆ«æ˜¯ `num_workers=0` æ—¶ï¼‰

**ä¿®å¤å»ºè®®**ï¼š
- ä½¿ç”¨ FFT äº’ç›¸å…³åŠ é€Ÿ
- æˆ–é¢„è®¡ç®—å¯¹é½åç§»å¹¶ç¼“å­˜

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

### ğŸŸ¢ P2-3ï¼šDeepFilterNet é€ä¸ªæ–‡ä»¶å¤„ç†

**ä½ç½®**ï¼š`data/prepare_dataset.py` ç¬¬ 272-305 è¡Œ

**é—®é¢˜æè¿°**ï¼šé€ä¸ªæ–‡ä»¶å¤„ç†ï¼Œæœªåˆ©ç”¨æ‰¹å¤„ç†èƒ½åŠ›

**å½±å“**ï¼šæ•°æ®å‡†å¤‡é˜¶æ®µå¯èƒ½è¾ƒæ…¢

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

### ğŸŸ¢ P2-4ï¼šç¼ºå°‘æ¢¯åº¦ç´¯ç§¯

**ä½ç½®**ï¼š`train.py` ç¬¬ 369-432 è¡Œ

**é—®é¢˜æè¿°**ï¼š
- å½“å‰æœªå®ç°æ¢¯åº¦ç´¯ç§¯
- å¦‚æœ GPU å†…å­˜ä¸è¶³ï¼Œæ— æ³•é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ¥æ¨¡æ‹Ÿæ›´å¤§çš„ batch size

**å½±å“**ï¼šé™åˆ¶äº†æœ€å¤§ batch sizeï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ

**ä¿®å¤å»ºè®®**ï¼š
```python
# æ·»åŠ æ¢¯åº¦ç´¯ç§¯æ”¯æŒ
accumulation_steps = config['training'].get('accumulation_steps', 1)

for batch_idx, (degraded, clean) in enumerate(pbar):
    losses = self.train_step(degraded, clean)
    
    # æ¯ accumulation_steps æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
    if (batch_idx + 1) % accumulation_steps == 0:
        self.optimizer_g.step()
        self.optimizer_d.step()
        self.optimizer_g.zero_grad()
        self.optimizer_d.zero_grad()
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

## ä¸‰ã€è®­ç»ƒæ•ˆæœé—®é¢˜

### ğŸ”´ P0-5ï¼šæ®‹å·®é—¨æ§è¿‡åº¦æŠ‘åˆ¶ä½éŸ³é‡ä¿¡å·ï¼ˆä¸¥é‡ï¼‰

**ä½ç½®**ï¼š`model/generator.py` ç¬¬ 386-393 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
# è½¯é—¨æ§ï¼šé™éŸ³æ®µï¼ˆ|residual| å¾ˆå°ï¼‰æ—¶ gateâ‰ˆ0ï¼Œé¿å…å™ªå£°æ³¨å…¥é™éŸ³.
gate = torch.tanh(torch.abs(residual) * 30.0)  # âŒ 30.0 è¿‡å¤§
x = residual + self.residual_scale * x * gate
# æ¸©å’Œè½¯é™åˆ¶ï¼Œä¿æŒå¤§éƒ¨åˆ†çº¿æ€§åŒºåŸŸï¼ŒåŒæ—¶æŠ‘åˆ¶è¿‡å¤§æŒ¯å¹…
x = x / (1.0 + 0.1 * torch.abs(x))  # âŒ è½¯é™åˆ¶å¯èƒ½è¿‡åº¦
if not self.training:
    x = torch.clamp(x, -1.0, 1.0)
```

**é—®é¢˜åˆ†æ**ï¼š
- `torch.tanh(abs * 30.0)` åœ¨ `abs(residual)` å¾ˆå°æ—¶å¿«é€Ÿæ¥è¿‘ 0
  - å½“ `abs(residual) < 0.033` æ—¶ï¼Œ`gate < 0.5`ï¼Œä¿®å¤è¢«æŠ‘åˆ¶ 50% ä»¥ä¸Š
  - å½“ `abs(residual) < 0.016` æ—¶ï¼Œ`gate < 0.2`ï¼Œä¿®å¤å‡ ä¹ä¸èµ·ä½œç”¨
- å¯¹äºä½éŸ³é‡æ®µï¼ˆå¦‚ -30dB èƒŒæ™¯å£°éŸ³ï¼‰ï¼Œé—¨æ§ä¼šè¿‡åº¦æŠ‘åˆ¶
- **DF å¯èƒ½åœ¨ä½éŸ³é‡æ®µä¹Ÿé€ æˆéŸ³è‰²æŸå¤±**ï¼Œä½†æ¨¡å‹å‡ ä¹ä¸ä¿®å¤
- è½¯é™åˆ¶ `x / (1.0 + 0.1 * abs(x))` å¯¹å¤§æŒ¯å¹…ä¿¡å·å‹ç¼©æ˜æ˜¾ï¼š
  - `abs(x) = 1.0` æ—¶ï¼Œè¾“å‡ºä¸º `1.0 / 1.1 = 0.91`ï¼ˆå‹ç¼© 9%ï¼‰
  - `abs(x) = 0.5` æ—¶ï¼Œè¾“å‡ºä¸º `0.5 / 1.05 = 0.476`ï¼ˆå‹ç¼© 4.8%ï¼‰

**å½±å“**ï¼š
- **ä¸¥é‡å½±å“ä¿®å¤æ•ˆæœ**
- ä½éŸ³é‡æ®µéŸ³è‰²ä¿®å¤ä¸è¶³
- å¤§éŸ³é‡æ®µè¿‡åº¦å‹ç¼©ï¼Œå¯èƒ½å¤±çœŸ

**ä¿®å¤å»ºè®®**ï¼š
```python
# æ–¹æ¡ˆ 1ï¼šè°ƒæ•´é—¨æ§å‚æ•°
gate = torch.sigmoid(torch.abs(residual) * 10.0)  # ä½¿ç”¨ sigmoidï¼Œæ›´å¹³æ»‘

# æ–¹æ¡ˆ 2ï¼šç®€åŒ–ä¸ºçº¿æ€§é—¨æ§
gate = torch.clamp(torch.abs(residual) * 5.0, 0.0, 1.0)

# æ–¹æ¡ˆ 3ï¼šç§»é™¤é—¨æ§ï¼Œç›´æ¥æ®‹å·®å­¦ä¹ 
x = residual + self.residual_scale * x

# æ–¹æ¡ˆ 4ï¼šè°ƒæ•´è½¯é™åˆ¶å‚æ•°
x = x / (1.0 + 0.02 * torch.abs(x))  # å‡å°å‹ç¼©ç³»æ•°
```

**ä¼˜å…ˆçº§**ï¼šğŸ”´ **P0 - éœ€è¦å®éªŒéªŒè¯æœ€ä½³å‚æ•°**

---

### ğŸŸ¡ P1-6ï¼šéªŒè¯æ—¶æœªè®¡ç®—å®Œæ•´æŸå¤±

**ä½ç½®**ï¼š`train.py` ç¬¬ 434-462 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
@torch.no_grad()
def validate(self) -> dict:
    """éªŒè¯"""
    self.generator.eval()
    
    total_loss = 0.0
    total_l1 = 0.0
    total_stft = 0.0
    count = 0
    
    for degraded, clean in self.val_loader:
        degraded = degraded.to(self.device)
        clean = clean.to(self.device)
        
        fake = self.generator(degraded)
        loss, losses = self.g_loss_fn(fake, clean)  # âŒ æœªè®¡ç®— GAN Loss
        
        total_loss += loss.item()
        total_l1 += losses['l1']
        total_stft += losses['stft']
        count += 1
```

**é—®é¢˜åˆ†æ**ï¼š
- éªŒè¯æ—¶åªè®¡ç®— L1 + STFT Lossï¼Œä¸è®¡ç®— GAN Loss
- è®­ç»ƒæ—¶ä½¿ç”¨ GAN Lossï¼Œä½†éªŒè¯æ—¶ä¸ç”¨
- **éªŒè¯æŒ‡æ ‡æ— æ³•å®Œæ•´åæ˜ æ¨¡å‹æ€§èƒ½**

**å½±å“**ï¼š
- éªŒè¯æŒ‡æ ‡å¯èƒ½ä¸å®é™…æ„ŸçŸ¥è´¨é‡ä¸ä¸€è‡´
- æ— æ³•é€šè¿‡éªŒè¯ Loss å‡†ç¡®è¯„ä¼°æ¨¡å‹

**ä¿®å¤å»ºè®®**ï¼š
```python
@torch.no_grad()
def validate(self) -> dict:
    """éªŒè¯"""
    self.generator.eval()
    self.discriminator.eval()  # åˆ¤åˆ«å™¨ä¹Ÿè®¾ä¸º eval æ¨¡å¼
    
    total_loss = 0.0
    total_l1 = 0.0
    total_stft = 0.0
    total_adv = 0.0
    count = 0
    
    train_config = self.config['training']
    use_gan = self.epoch >= train_config['gan_start_epoch']
    
    for degraded, clean in self.val_loader:
        degraded = degraded.to(self.device)
        clean = clean.to(self.device)
        
        fake = self.generator(degraded)
        
        if use_gan:
            fake_out, fake_feats = self.discriminator(fake)
            _, real_feats = self.discriminator(clean)
            
            loss, losses = self.g_loss_fn(
                fake, clean,
                disc_fake_outputs=fake_out,
                disc_fake_features=fake_feats,
                disc_real_features=real_feats,
            )
        else:
            loss, losses = self.g_loss_fn(fake, clean)
        
        total_loss += loss.item()
        total_l1 += losses.get('l1', 0)
        total_stft += losses.get('stft', 0)
        total_adv += losses.get('adv', 0)
        count += 1
    
    self.generator.train()
    self.discriminator.train()
    
    return {
        'val_loss': total_loss / max(count, 1),
        'val_l1': total_l1 / max(count, 1),
        'val_stft': total_stft / max(count, 1),
        'val_adv': total_adv / max(count, 1),
    }
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®ä¿®å¤**

---

### ğŸŸ¡ P1-7ï¼šSTFT Loss é«˜é¢‘åŠ æƒå¯èƒ½è¿‡åº¦

**ä½ç½®**ï¼š`model/losses.py` ç¬¬ 51-63 è¡Œï¼Œ`configs/default.yaml` ç¬¬ 104-105 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
def _get_frequency_weight(self, freq_bins: int, fft_size: int, device: torch.device) -> torch.Tensor:
    weight = torch.ones(freq_bins, device=device)
    
    hf_start_bin = int(self.hf_cutoff * fft_size / self.sample_rate)  # 3000 Hz
    if hf_start_bin < freq_bins:
        bins = torch.arange(freq_bins, device=device, dtype=torch.float32)
        transition_width = max(1, (freq_bins - hf_start_bin) // 4)
        sigmoid_input = (bins - hf_start_bin) / transition_width
        smooth_ramp = torch.sigmoid(sigmoid_input)
        weight = 1.0 + (self.hf_weight - 1.0) * smooth_ramp  # hf_weight=2.0
    
    return weight.view(1, -1, 1)
```

**é…ç½®**ï¼š
```yaml
stft_config:
  hf_weight: 2.0       # é«˜é¢‘åŠ æƒå€æ•°
  hf_cutoff: 3000      # é«˜é¢‘èµ·å§‹é¢‘ç‡ (Hz)
```

**é—®é¢˜åˆ†æ**ï¼š
- 3000 Hz ä»¥ä¸Šé¢‘ç‡æŸå¤±æƒé‡åŠ å€
- å¯¹äºè¯­éŸ³ï¼Œ3000 Hz ä»¥ä¸Šä¸»è¦æ˜¯æ‘©æ“¦éŸ³å’Œé½¿éŸ³
- æƒé‡åŠ å€å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡åº¦å…³æ³¨é«˜é¢‘ï¼Œå¿½ç•¥ä¸­ä½é¢‘éŸ³è‰²

**å½±å“**ï¼šå¯èƒ½å¯¼è‡´ä¸­ä½é¢‘ä¿®å¤ä¸è¶³

**ä¿®å¤å»ºè®®**ï¼š
```yaml
# æ–¹æ¡ˆ 1ï¼šé™ä½é«˜é¢‘æƒé‡
stft_config:
  hf_weight: 1.5       # é™ä½åˆ° 1.5
  hf_cutoff: 3000

# æ–¹æ¡ˆ 2ï¼šæé«˜é«˜é¢‘èµ·å§‹é¢‘ç‡
stft_config:
  hf_weight: 2.0
  hf_cutoff: 4000      # æé«˜åˆ° 4000 Hz
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®å®éªŒéªŒè¯**

---

### ğŸŸ¡ P1-8ï¼šæŸå¤±æƒé‡å¯èƒ½ä¸å¹³è¡¡

**ä½ç½®**ï¼š`configs/default.yaml` ç¬¬ 91-95 è¡Œ

**é—®é¢˜é…ç½®**ï¼š
```yaml
loss_weights:
  l1: 5.0              # æ—¶åŸŸ
  multi_stft: 5.0      # é¢‘åŸŸ
  adversarial: 0.5     # GAN
  feature_matching: 1.5 # ç‰¹å¾åŒ¹é…
```

**é—®é¢˜åˆ†æ**ï¼š
- L1 å’Œ STFT æƒé‡ç›¸åŒï¼ˆ5.0ï¼‰ï¼Œä½† STFT Loss é€šå¸¸æ¯” L1 å¤§å¾—å¤š
- Adversarial Loss æƒé‡è¿‡å°ï¼ˆ0.5ï¼‰ï¼ŒGAN å¯èƒ½è®­ç»ƒä¸è¶³
- æ€»æƒé‡ï¼š5.0 (L1) + 5.0 (STFT) + 0.5 (Adv) + 1.5 (FM) = 12.0
  - L1 å  41.7%
  - STFT å  41.7%
  - Adv å  4.2%
  - FM å  12.5%

**å½±å“**ï¼š
- GAN è®­ç»ƒä¸è¶³ï¼Œæ„ŸçŸ¥è´¨é‡å¯èƒ½ä¸ä½³
- L1 + STFT å æ¯”è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦å¹³æ»‘

**ä¿®å¤å»ºè®®**ï¼š
```yaml
# æ–¹æ¡ˆ 1ï¼šå¢å¼º GAN
loss_weights:
  l1: 3.0
  multi_stft: 3.0
  adversarial: 1.0      # å¢åŠ 
  feature_matching: 2.0  # å¢åŠ 

# æ–¹æ¡ˆ 2ï¼šé™ä½ L1/STFT
loss_weights:
  l1: 2.0               # é™ä½
  multi_stft: 2.0       # é™ä½
  adversarial: 1.0
  feature_matching: 1.5
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ **P1 - å»ºè®®å®éªŒéªŒè¯**

---

### ğŸŸ¢ P2-5ï¼šè½¬ç½®å·ç§¯è¾“å‡ºé•¿åº¦å¯¹é½å¯èƒ½ä¸å‡†ç¡®

**ä½ç½®**ï¼š`model/generator.py` ç¬¬ 380-384 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
# å¯¹é½é•¿åº¦
if x.size(-1) > input_len:
    x = x[:, :, :input_len]  # ç›´æ¥è£å‰ª
elif x.size(-1) < input_len:
    x = F.pad(x, (0, input_len - x.size(-1)))  # é›¶å¡«å……
```

**å½±å“**ï¼šå¯èƒ½äº§ç”Ÿè½»å¾®çš„éŸ³é¢‘ä¼ªå½±

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

## å››ã€ä»£ç é€»è¾‘é”™è¯¯

### ğŸŸ¢ P2-6ï¼šæœªä½¿ç”¨çš„å˜é‡

**ä½ç½®**ï¼š`train.py` ç¬¬ 275 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
else:
    train_sampler = None
    val_sampler = None  # âŒ å®šä¹‰äº†ä½†æœªä½¿ç”¨
```

**ä¿®å¤å»ºè®®**ï¼šåˆ é™¤è¯¥è¡Œ

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - ä»£ç æ¸…ç†**

---

### ğŸŸ¢ P2-7ï¼šONNX å¯¼å‡ºæ—¶å‡è®¾çŠ¶æ€å½¢çŠ¶

**ä½ç½®**ï¼š`export_onnx.py` ç¬¬ 144-154 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
hidden_size = model.channels[-1]  # âŒ å‡è®¾ hidden_size == channels[-1]
```

**é—®é¢˜åˆ†æ**ï¼š
- GRU/LSTM çš„ hidden_size å¯èƒ½ä¸ç­‰äº channels[-1]
- æŸ¥çœ‹ `model/generator.py` ç¬¬ 201-216 è¡Œï¼š
  ```python
  self.gru = nn.GRU(
      channels,      # input_size
      hidden_size,   # hidden_sizeï¼ˆå¯èƒ½ä¸ç­‰äº channelsï¼‰
      num_layers=num_layers,
  ```

**å½±å“**ï¼šæµå¼å¯¼å‡ºæ—¶çŠ¶æ€å½¢çŠ¶å¯èƒ½é”™è¯¯

**ä¿®å¤å»ºè®®**ï¼š
```python
if hasattr(model.bottleneck, "gru"):
    hidden_size = model.bottleneck.gru.hidden_size
elif hasattr(model.bottleneck, "lstm"):
    hidden_size = model.bottleneck.lstm.hidden_size
```

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å»ºè®®ä¿®å¤**

---

## äº”ã€å®ç°ç»†èŠ‚é—®é¢˜

### ğŸŸ¢ P2-8ï¼šå¼‚å¸¸å¤„ç†è¿‡äºå®½æ³›

**ä½ç½®**ï¼š`data/prepare_dataset.py` ç¬¬ 264-269 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
except Exception as e:  # âŒ æ•è·æ‰€æœ‰å¼‚å¸¸
    print(f"Error processing {clean_path}: {e}")
    return None
```

**ä¿®å¤å»ºè®®**ï¼šåŒºåˆ†å¼‚å¸¸ç±»å‹ï¼Œè®°å½•è¯¦ç»†æ—¥å¿—

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

### ğŸŸ¢ P2-9ï¼šé…ç½®éªŒè¯ä¸å®Œæ•´

**ä½ç½®**ï¼š`train.py` ç¬¬ 72-73 è¡Œ

**é—®é¢˜ä»£ç **ï¼š
```python
with open(config_path) as f:
    self.config = yaml.safe_load(f)  # âŒ æ²¡æœ‰éªŒè¯å¿…éœ€å­—æ®µ
```

**ä¿®å¤å»ºè®®**ï¼šæ·»åŠ é…ç½® schema éªŒè¯

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

### ğŸŸ¢ P2-10ï¼šRust é›†æˆä¸­ä¸Šä¸‹æ–‡ç¼“å†²å¯èƒ½ä¸è¶³

**ä½ç½®**ï¼š`rust_integration/timbre_restore.rs` ç¬¬ 42ã€68 è¡Œ

**é—®é¢˜æè¿°**ï¼š
- `context_size` é»˜è®¤ 256 å¯èƒ½ä¸è¶³ä»¥è¦†ç›–æ‰€æœ‰å·ç§¯å±‚çš„æ„Ÿå—é‡
- U-Net æœ‰å¤šå±‚å·ç§¯ï¼Œæ„Ÿå—é‡å¯èƒ½è¶…è¿‡ 256 æ ·æœ¬

**ä¿®å¤å»ºè®®**ï¼šæ ¹æ®æ¨¡å‹é…ç½®è®¡ç®—æ„Ÿå—é‡ï¼Œæˆ–å¢åŠ é»˜è®¤å€¼åˆ° 512-1024

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ **P2 - å¯é€‰ä¼˜åŒ–**

---

## å…­ã€ä¿®å¤ä¼˜å…ˆçº§

### ğŸ”´ P0 - å¿…é¡»ç«‹å³ä¿®å¤ï¼ˆå½±å“è®­ç»ƒæ­£ç¡®æ€§ï¼‰

| é—®é¢˜ç¼–å· | é—®é¢˜æè¿° | ä½ç½® | å½±å“ | éš¾åº¦ | æ—¶é—´ |
|---------|---------|------|------|------|------|
| P0-1 | æ•°æ®å¯¹é½åéšæœºè£å‰ªç ´åå¯¹é½ | `data/dataset.py` 145-160 | ä¸¥é‡å½±å“è®­ç»ƒè´¨é‡ | ä½ | 10åˆ†é’Ÿ |
| P0-2 | æ•°æ®å‡†å¤‡æµç¨‹éœ€éªŒè¯ | `data/prepare_dataset.py` 216-305 | éœ€ç¡®è®¤æ•°æ®é›†æ­£ç¡®æ€§ | ä½ | 30åˆ†é’Ÿ |
| P0-3 | å­¦ä¹ ç‡è°ƒåº¦å™¨ä½œç”¨åŸŸé”™è¯¯ | `train.py` 568-573 | ä¸¥é‡å½±å“è®­ç»ƒæ•ˆæœ | ä½ | 10åˆ†é’Ÿ |
| P0-4 | num_workers=0 é™åˆ¶è®­ç»ƒé€Ÿåº¦ | `train.py` 282-299 | è®­ç»ƒé€Ÿåº¦æ…¢2-5å€ | ä½ | 5åˆ†é’Ÿ |
| P0-5 | æ®‹å·®é—¨æ§è¿‡åº¦æŠ‘åˆ¶ | `model/generator.py` 386-393 | å½±å“ä¿®å¤æ•ˆæœ | ä¸­ | éœ€å®éªŒ |

**P0 æ€»è®¡ä¿®å¤æ—¶é—´**ï¼šçº¦ 1 å°æ—¶ï¼ˆä¸å«å®éªŒéªŒè¯ï¼‰

---

### ğŸŸ¡ P1 - å»ºè®®ä¿®å¤ï¼ˆå½±å“è®­ç»ƒæ•ˆæœæˆ–æ•ˆç‡ï¼‰

| é—®é¢˜ç¼–å· | é—®é¢˜æè¿° | ä½ç½® | å½±å“ | éš¾åº¦ | æ—¶é—´ |
|---------|---------|------|------|------|------|
| P1-1 | æ··å“æ·»åŠ åèƒ½é‡å½’ä¸€åŒ–ç ´å SNR | `data/prepare_dataset.py` 125-136 | æ•°æ®é›† SNR ä¸å‡†ç¡® | ä½ | 10åˆ†é’Ÿ |
| P1-2 | å™ªå£°æ·»åŠ åå½’ä¸€åŒ–ç ´å SNR | `data/prepare_dataset.py` 139-167 | æ•°æ®é›† SNR ä¸å‡†ç¡® | ä½ | 10åˆ†é’Ÿ |
| P1-3 | æ•°æ®å¢å¼ºæ„ä¹‰ä¸å¤§ | `data/dataset.py` 175-179 | æ•°æ®å¢å¼ºæ•ˆæœæœ‰é™ | ä½ | 5åˆ†é’Ÿ |
| P1-4 | éªŒè¯é›† drop_last æµªè´¹æ•°æ® | `train.py` 297 | éªŒè¯æ•°æ®æµªè´¹ | ä½ | 1åˆ†é’Ÿ |
| P1-5 | GradScaler æœªæŒ‡å®š device | `train.py` 96-97 | CPU è®­ç»ƒå¯èƒ½å‡ºé”™ | ä½ | 2åˆ†é’Ÿ |
| P1-6 | éªŒè¯æ—¶æœªè®¡ç®—å®Œæ•´æŸå¤± | `train.py` 434-462 | éªŒè¯æŒ‡æ ‡ä¸å®Œæ•´ | ä½ | 20åˆ†é’Ÿ |
| P1-7 | STFT é«˜é¢‘åŠ æƒå¯èƒ½è¿‡åº¦ | `configs/default.yaml` 104-105 | å¯èƒ½å½±å“ä¿®å¤æ•ˆæœ | ä½ | éœ€å®éªŒ |
| P1-8 | æŸå¤±æƒé‡å¯èƒ½ä¸å¹³è¡¡ | `configs/default.yaml` 91-95 | å¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ | ä½ | éœ€å®éªŒ |

**P1 æ€»è®¡ä¿®å¤æ—¶é—´**ï¼šçº¦ 1 å°æ—¶ï¼ˆä¸å«å®éªŒéªŒè¯ï¼‰

---

### ğŸŸ¢ P2 - å¯é€‰ä¼˜åŒ–ï¼ˆä»£ç è´¨é‡å’Œæ€§èƒ½ä¼˜åŒ–ï¼‰

| é—®é¢˜ç¼–å· | é—®é¢˜æè¿° | å½±å“ |
|---------|---------|------|
| P2-1 | å¯¹é½ä¼°è®¡ç®—æ³•ä¸å¤Ÿé²æ£’ | å¯¹é½ä¸å‡†ç¡® |
| P2-2 | æ•°æ®å¯¹é½è®¡ç®—å¼€é”€å¤§ | æ•°æ®åŠ è½½æ…¢ |
| P2-3 | DeepFilterNet é€ä¸ªæ–‡ä»¶å¤„ç† | æ•°æ®å‡†å¤‡æ…¢ |
| P2-4 | ç¼ºå°‘æ¢¯åº¦ç´¯ç§¯ | é™åˆ¶æœ€å¤§ batch size |
| P2-5 | è½¬ç½®å·ç§¯è¾“å‡ºé•¿åº¦å¯¹é½ | å¯èƒ½æœ‰è½»å¾®ä¼ªå½± |
| P2-6 | æœªä½¿ç”¨çš„å˜é‡ | ä»£ç å†—ä½™ |
| P2-7 | ONNX å¯¼å‡ºå‡è®¾çŠ¶æ€å½¢çŠ¶ | æµå¼å¯¼å‡ºå¯èƒ½é”™è¯¯ |
| P2-8 | å¼‚å¸¸å¤„ç†è¿‡äºå®½æ³› | éš¾ä»¥è°ƒè¯• |
| P2-9 | é…ç½®éªŒè¯ä¸å®Œæ•´ | é”™è¯¯æç¤ºä¸åŠæ—¶ |
| P2-10 | Rust ä¸Šä¸‹æ–‡ç¼“å†²å¯èƒ½ä¸è¶³ | æµå¼æ¨ç†å¯èƒ½ä¸¢å¤±ä¿¡æ¯ |

**P2 æ€»è®¡**ï¼š10 ä¸ªé—®é¢˜ï¼Œå¯é€æ­¥ä¼˜åŒ–

---

## ä¸ƒã€æ€»ä½“è¯„ä»·

### ä»£ç è´¨é‡ï¼šè‰¯å¥½ â†’ éœ€è¦ä¿®å¤

**ä¼˜ç‚¹**ï¼š
- âœ… æ¶æ„æ¸…æ™°ï¼Œæ¨¡å—åŒ–è‰¯å¥½
- âœ… åŠŸèƒ½å®Œæ•´ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œ ONNX å¯¼å‡º
- âœ… ä»£ç æ³¨é‡Šå……åˆ†

**ä¸¥é‡é—®é¢˜**ï¼š
- âŒ æ•°æ®å¯¹é½åéšæœºè£å‰ªç ´åå¯¹é½ï¼ˆP0-1ï¼‰
- âŒ å­¦ä¹ ç‡è°ƒåº¦å™¨ä½œç”¨åŸŸé”™è¯¯ï¼ˆP0-3ï¼‰
- âŒ num_workers=0 ä¸¥é‡é™åˆ¶è®­ç»ƒé€Ÿåº¦ï¼ˆP0-4ï¼‰
- âŒ æ®‹å·®é—¨æ§å¯èƒ½è¿‡åº¦æŠ‘åˆ¶ï¼ˆP0-5ï¼‰

**ä¸­ç­‰é—®é¢˜**ï¼š
- âš ï¸ æ•°æ®å‡†å¤‡é˜¶æ®µ SNR ä¸å‡†ç¡®ï¼ˆP1-1, P1-2ï¼‰
- âš ï¸ éªŒè¯æŒ‡æ ‡ä¸å®Œæ•´ï¼ˆP1-6ï¼‰
- âš ï¸ æŸå¤±æƒé‡å¯èƒ½ä¸å¹³è¡¡ï¼ˆP1-7, P1-8ï¼‰

### æ–¹æ¡ˆå¯è¡Œæ€§ï¼šä¸­ç­‰

- æ•´ä½“æ¶æ„åˆç†ï¼Œä½†å­˜åœ¨ä¸¥é‡çš„å®ç°é—®é¢˜
- **å¿…é¡»ä¿®å¤ P0 é—®é¢˜æ‰èƒ½å¼€å§‹è®­ç»ƒ**
- P1 é—®é¢˜ä¼šå½±å“è®­ç»ƒæ•ˆæœï¼Œå»ºè®®ä¿®å¤

### é£é™©è¯„ä¼°ï¼šä¸­é«˜

- **æ•°æ®é›†å‡†ç¡®æ€§**ï¼šå­˜åœ¨ä¸¥é‡é—®é¢˜ï¼ˆP0-1, P0-2, P1-1, P1-2ï¼‰
- **è®­ç»ƒæ•ˆç‡**ï¼šå­˜åœ¨ä¸¥é‡ç“¶é¢ˆï¼ˆP0-4, P0-3ï¼‰
- **è®­ç»ƒæ•ˆæœ**ï¼šå¯èƒ½å—é™ï¼ˆP0-5, P1-6, P1-7, P1-8ï¼‰

---

## å…«ã€ä¿®å¤å»ºè®®

### ç¬¬ä¸€é˜¶æ®µï¼šP0 é—®é¢˜ä¿®å¤ï¼ˆå¿…é¡»ï¼‰

**æ—¶é—´**ï¼š1-2 å°æ—¶

1. **ä¿®å¤æ•°æ®å¯¹é½é€»è¾‘**ï¼ˆP0-1ï¼‰
   - å°†éšæœºè£å‰ªæ”¹ä¸ºå›ºå®šè£å‰ª
   - éªŒè¯å¯¹é½æ•ˆæœ

2. **éªŒè¯æ•°æ®å‡†å¤‡æµç¨‹**ï¼ˆP0-2ï¼‰
   - ç¡®è®¤ DF å¤„ç†åæ–‡ä»¶æ­£ç¡®æ›´æ–°
   - æ£€æŸ¥ clean vs degraded æ˜¯å¦å¯¹åº”

3. **ä¿®å¤å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼ˆP0-3ï¼‰
   - å°†è°ƒåº¦å™¨æ›´æ–°ç§»åˆ°å¾ªç¯å†…
   - éªŒè¯å­¦ä¹ ç‡å˜åŒ–

4. **å¯ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½**ï¼ˆP0-4ï¼‰
   - è®¾ç½® `num_workers=4`
   - å¯ç”¨ `pin_memory=True`
   - å¢åŠ  `prefetch_factor=2`

5. **è°ƒæ•´æ®‹å·®é—¨æ§å‚æ•°**ï¼ˆP0-5ï¼‰
   - å°è¯•ä¸åŒçš„é—¨æ§å‚æ•°
   - è¿›è¡Œå°è§„æ¨¡å®éªŒéªŒè¯

### ç¬¬äºŒé˜¶æ®µï¼šP1 é—®é¢˜ä¿®å¤ï¼ˆå»ºè®®ï¼‰

**æ—¶é—´**ï¼š1-2 å°æ—¶

1. **ä¿®å¤ SNR è®¡ç®—**ï¼ˆP1-1, P1-2ï¼‰
   - ç§»é™¤æ··å“åçš„èƒ½é‡å½’ä¸€åŒ–
   - è°ƒæ•´å™ªå£°æ·»åŠ åçš„å½’ä¸€åŒ–ç­–ç•¥

2. **å®Œå–„éªŒè¯æµç¨‹**ï¼ˆP1-6ï¼‰
   - éªŒè¯æ—¶è®¡ç®—å®Œæ•´æŸå¤±
   - æ·»åŠ æ›´å¤šéªŒè¯æŒ‡æ ‡

3. **è°ƒæ•´æŸå¤±æƒé‡**ï¼ˆP1-7, P1-8ï¼‰
   - è¿›è¡Œæ¶ˆèå®éªŒ
   - æ‰¾åˆ°æœ€ä½³æƒé‡é…ç½®

4. **å…¶ä»– P1 é—®é¢˜**ï¼ˆP1-3, P1-4, P1-5ï¼‰
   - å¿«é€Ÿä¿®å¤

### ç¬¬ä¸‰é˜¶æ®µï¼šP2 é—®é¢˜ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

**æ—¶é—´**ï¼šæ ¹æ®éœ€è¦é€æ­¥ä¼˜åŒ–

---

## ä¹ã€éªŒè¯æ¸…å•

ä¿®å¤åéœ€è¦éªŒè¯çš„é¡¹ç›®ï¼š

### æ•°æ®é›†éªŒè¯
- [ ] æ£€æŸ¥ clean vs degraded æ˜¯å¦å¯¹åº”
- [ ] éªŒè¯å¯¹é½æ•ˆæœï¼ˆå¯è§†åŒ–æ³¢å½¢ï¼‰
- [ ] æ£€æŸ¥ SNR æ˜¯å¦å‡†ç¡®
- [ ] ç»Ÿè®¡æ•°æ®é›†è´¨é‡æŒ‡æ ‡

### è®­ç»ƒéªŒè¯
- [ ] å°è§„æ¨¡è®­ç»ƒï¼ˆ100 æ ·æœ¬ï¼Œ5 epochsï¼‰
- [ ] æ£€æŸ¥å­¦ä¹ ç‡å˜åŒ–æ›²çº¿
- [ ] ç›‘æ§ GPU åˆ©ç”¨ç‡
- [ ] éªŒè¯ Loss æ˜¯å¦æ­£å¸¸ä¸‹é™

### æ•ˆæœéªŒè¯
- [ ] å¯¹æ¯”ä¿®å¤å‰åçš„éŸ³é¢‘è´¨é‡
- [ ] æ£€æŸ¥ä½éŸ³é‡æ®µä¿®å¤æ•ˆæœ
- [ ] æ£€æŸ¥é«˜éŸ³é‡æ®µæ˜¯å¦å¤±çœŸ
- [ ] å¯¹æ¯”ä¸åŒæŸå¤±æƒé‡çš„æ•ˆæœ

---

## åã€æ€»ç»“

### å…³é”®é—®é¢˜

**æ•°æ®é›†å‡†ç¡®æ€§**ï¼š
- ğŸ”´ æ•°æ®å¯¹é½åéšæœºè£å‰ªç ´åå¯¹é½ï¼ˆP0-1ï¼‰
- ğŸ”´ éœ€éªŒè¯æ•°æ®å‡†å¤‡æµç¨‹ï¼ˆP0-2ï¼‰
- ğŸŸ¡ SNR è®¡ç®—ä¸å‡†ç¡®ï¼ˆP1-1, P1-2ï¼‰

**è®­ç»ƒæ•ˆç‡**ï¼š
- ğŸ”´ num_workers=0 ä¸¥é‡é™åˆ¶é€Ÿåº¦ï¼ˆP0-4ï¼‰
- ğŸ”´ å­¦ä¹ ç‡è°ƒåº¦å™¨é”™è¯¯ï¼ˆP0-3ï¼‰

**è®­ç»ƒæ•ˆæœ**ï¼š
- ğŸ”´ æ®‹å·®é—¨æ§å¯èƒ½è¿‡åº¦æŠ‘åˆ¶ï¼ˆP0-5ï¼‰
- ğŸŸ¡ éªŒè¯æŒ‡æ ‡ä¸å®Œæ•´ï¼ˆP1-6ï¼‰
- ğŸŸ¡ æŸå¤±æƒé‡å¯èƒ½ä¸å¹³è¡¡ï¼ˆP1-7, P1-8ï¼‰

### å»ºè®®è¡ŒåŠ¨

**ç«‹å³æ‰§è¡Œ**ï¼š
1. ä¿®å¤ P0 æ‰€æœ‰é—®é¢˜ï¼ˆ~2 å°æ—¶ï¼‰
2. å°è§„æ¨¡è®­ç»ƒéªŒè¯ï¼ˆ~1 å°æ—¶ï¼‰
3. ä¿®å¤ P1 ä¸»è¦é—®é¢˜ï¼ˆ~2 å°æ—¶ï¼‰
4. å…¨é‡è®­ç»ƒ

**ä¸å»ºè®®**ï¼š
- âŒ åœ¨æœªä¿®å¤ P0 é—®é¢˜å‰å¼€å§‹è®­ç»ƒ
- âŒ å¿½ç•¥æ•°æ®é›†å‡†ç¡®æ€§é—®é¢˜

---

**å®¡è®¡å®Œæˆ**

æ­¤æŠ¥å‘Šæ¶µç›–äº†æ‰€æœ‰å½±å“æ•°æ®é›†å‡†ç¡®æ€§ã€è®­ç»ƒæ•ˆç‡å’Œè®­ç»ƒæ•ˆæœçš„å…³é”®é—®é¢˜ã€‚å»ºè®®å¼€å‘å›¢é˜ŸæŒ‰ä¼˜å…ˆçº§é€ä¸ªä¿®å¤ï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶æ®µè¿›è¡ŒéªŒè¯ã€‚


# æœ€ç»ˆä¸¥æ ¼å®¡è®¡æŠ¥å‘Š - å…³é”®é—®é¢˜æ¸…å•

**å®¡è®¡ç›®æ ‡**ï¼šç¡®ä¿æ•°æ®é›†å‡†ç¡®ã€æ€§èƒ½ä¼˜ç§€ã€æ¨¡å‹æœ‰æ•ˆ  
**å®¡è®¡èŒƒå›´**ï¼šä»…åˆ—å‡ºå¿…é¡»ä¿®å¤çš„é—®é¢˜  
**å®¡è®¡æ—¥æœŸ**ï¼š2024å¹´

---

## ğŸ”´ ä¸¥é‡é—®é¢˜ï¼ˆå¿…é¡»ç«‹å³ä¿®å¤ï¼‰

### é—®é¢˜ 1ï¼šé…ç½®ä¸ä»£ç ä¸ä¸€è‡´ - æŸå¤±æƒé‡

**ä½ç½®**ï¼š
- `configs/default.yaml` Line 92-95
- `model/losses.py` Line 208-209, 228-229

**é—®é¢˜**ï¼š
```yaml
# configs/default.yaml
loss_weights:
  l1: 3.0              # é…ç½®æ–‡ä»¶ä¸­æ˜¯ 3.0
  multi_stft: 3.0      # é…ç½®æ–‡ä»¶ä¸­æ˜¯ 3.0
```

```python
# model/losses.py Line 208-209
def __init__(
    self,
    l1_weight: float = 15.0,       # âŒ ä»£ç é»˜è®¤å€¼æ˜¯ 15.0
    stft_weight: float = 2.0,      # âŒ ä»£ç é»˜è®¤å€¼æ˜¯ 2.0
```

**é—®é¢˜åˆ†æ**ï¼š
- ä»£ç é»˜è®¤å€¼ä¸é…ç½®æ–‡ä»¶ä¸ä¸€è‡´
- å¦‚æœé…ç½®æ–‡ä»¶ä¼ å‚æœ‰è¯¯ï¼Œä¼šä½¿ç”¨é”™è¯¯çš„é»˜è®¤å€¼
- æŸå¤±æƒé‡å¯¹è®­ç»ƒæ•ˆæœå½±å“å·¨å¤§

**å½±å“**ï¼šå¯èƒ½å¯¼è‡´è®­ç»ƒæ•ˆæœä¸ç¬¦åˆé¢„æœŸ

**ä¿®å¤**ï¼šå°†ä»£ç é»˜è®¤å€¼æ”¹ä¸ºä¸é…ç½®æ–‡ä»¶ä¸€è‡´ï¼Œæˆ–ç¡®ä¿é…ç½®æ–‡ä»¶æ­£ç¡®ä¼ é€’

---

### é—®é¢˜ 2ï¼šSTFT Loss é«˜é¢‘åŠ æƒé…ç½®ä¸ä¸€è‡´

**ä½ç½®**ï¼š
- `configs/default.yaml` Line 104
- `model/losses.py` Line 228-229

**é—®é¢˜**ï¼š
```yaml
# configs/default.yaml
hf_weight: 1.5       # é…ç½®æ–‡ä»¶ä¸­æ˜¯ 1.5
hf_cutoff: 3000
```

```python
# model/losses.py Line 228-229
stft_config = {
    'hf_weight': 2.0,      # âŒ ä»£ç é»˜è®¤å€¼æ˜¯ 2.0
    'hf_cutoff': 3000,
}
```

**å½±å“**ï¼šå¦‚æœé…ç½®ä¼ é€’æœ‰è¯¯ï¼Œä¼šä½¿ç”¨é”™è¯¯çš„é«˜é¢‘åŠ æƒ

**ä¿®å¤**ï¼šç»Ÿä¸€é…ç½®å’Œä»£ç é»˜è®¤å€¼

---

### é—®é¢˜ 3ï¼šskip_existing ä¸æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§

**ä½ç½®**ï¼š`data/prepare_dataset.py` Line 246, 290

**é—®é¢˜**ï¼š
```python
# Line 246
if skip_existing and clean_out.exists() and degraded_out.exists():
    continue  # âŒ åªæ£€æŸ¥å­˜åœ¨ï¼Œä¸æ£€æŸ¥å®Œæ•´æ€§

# Line 290
if skip_existing and output_path.exists():
    continue  # âŒ åªæ£€æŸ¥å­˜åœ¨ï¼Œä¸æ£€æŸ¥å®Œæ•´æ€§
```

**é—®é¢˜åˆ†æ**ï¼š
- å¦‚æœå¤„ç†ä¸­æ–­ï¼Œå¯èƒ½ç•™ä¸‹ä¸å®Œæ•´çš„æ–‡ä»¶ï¼ˆ0å­—èŠ‚æˆ–éƒ¨åˆ†å†™å…¥ï¼‰
- ä¸‹æ¬¡è¿è¡Œä¼šè·³è¿‡è¿™äº›æŸåçš„æ–‡ä»¶
- **å¯¼è‡´è®­ç»ƒé›†åŒ…å«æŸåæ•°æ®æˆ–æ ·æœ¬ç¼ºå¤±**

**å½±å“**ï¼šå¯èƒ½å¯¼è‡´è®­ç»ƒå¤±è´¥æˆ–æ•ˆæœå·®

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# æ–¹æ¡ˆ 1ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°
if skip_existing and clean_out.exists() and degraded_out.exists():
    if clean_out.stat().st_size > 1000 and degraded_out.stat().st_size > 1000:
        continue
    # å¦åˆ™é‡æ–°ç”Ÿæˆ

# æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼ˆæ¨èï¼‰
tmp_clean = str(clean_out) + ".tmp"
sf.write(tmp_clean, clean_seg, target_sr)
os.rename(tmp_clean, clean_out)  # åŸå­æ“ä½œ
```

---

### é—®é¢˜ 4ï¼šæ•°æ®å‡†å¤‡ç»“æœåˆ—è¡¨å¯èƒ½è¢«é”™è¯¯è¿‡æ»¤

**ä½ç½®**ï¼š`data/prepare_dataset.py` Line 471-482

**é—®é¢˜ä»£ç **ï¼š
```python
# æ›´æ–° results ä¸­çš„ degraded è·¯å¾„
filtered = []
for r in results:
    name = Path(r['degraded']).name
    if name in failed_df:  # âŒ 'in' æ“ä½œå¯¹åˆ—è¡¨æ•ˆç‡ä½
        continue
    degraded_path = degraded_dir / name
    if not degraded_path.exists():  # âŒ è¿™é‡Œå¯èƒ½æ¼æ‰æ–‡ä»¶
        continue
    r['degraded'] = str(degraded_path)
    filtered.append(r)
results = filtered
```

**é—®é¢˜åˆ†æ**ï¼š
1. `failed_df` æ˜¯åˆ—è¡¨ï¼Œ`in` æ“ä½œæ˜¯ O(n)ï¼Œæ•ˆç‡ä½
2. æ›´ä¸¥é‡çš„æ˜¯ï¼š`r['degraded']` æŒ‡å‘ `noisy_dir`ï¼Œä½†æ£€æŸ¥çš„æ˜¯ `degraded_dir`
   - å¦‚æœ DF å¤„ç†äº†æ–‡ä»¶ä½† results é‡Œçš„è·¯å¾„è¿˜æ˜¯ noisy_dirï¼Œä¼šæ‰¾ä¸åˆ°æ–‡ä»¶

**å½±å“**ï¼šå¯èƒ½å¯¼è‡´å¤§é‡æ ·æœ¬è¢«é”™è¯¯è¿‡æ»¤ï¼Œè®­ç»ƒé›†å˜å°

**ä¿®å¤**ï¼š
```python
# è½¬ä¸ºé›†åˆæé«˜æ•ˆç‡
failed_df_set = set(failed_df)

filtered = []
for r in results:
    name = Path(r['degraded']).name
    if name in failed_df_set:
        continue
    degraded_path = degraded_dir / name
    if not degraded_path.exists():
        print(f"Warning: {degraded_path} not found, skipping")  # æ·»åŠ æ—¥å¿—
        continue
    r['degraded'] = str(degraded_path)
    filtered.append(r)

print(f"Filtered: {len(results)} -> {len(filtered)} samples")  # æ·»åŠ æ—¥å¿—
results = filtered
```

---

### é—®é¢˜ 5ï¼šæ•°æ®å‡†å¤‡æµç¨‹ç¼ºå°‘æœ€ç»ˆéªŒè¯

**ä½ç½®**ï¼š`data/prepare_dataset.py` Line 500-509

**é—®é¢˜**ï¼š
- ç”Ÿæˆ train.txt å’Œ val.txt åï¼Œæ²¡æœ‰éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨
- å¦‚æœè·¯å¾„é”™è¯¯æˆ–æ–‡ä»¶è¢«åˆ é™¤ï¼Œè®­ç»ƒæ—¶æ‰ä¼šæŠ¥é”™

**å½±å“**ï¼šæµªè´¹æ—¶é—´ï¼Œè®­ç»ƒå¯åŠ¨åæ‰å‘ç°æ•°æ®é—®é¢˜

**ä¿®å¤**ï¼š
```python
# ä¿å­˜æ–‡ä»¶åˆ—è¡¨å‰éªŒè¯
print("\nValidating generated file pairs...")
invalid = 0
for r in train_results + val_results:
    if not Path(r['degraded']).exists():
        print(f"Missing: {r['degraded']}")
        invalid += 1
    if not Path(r['clean']).exists():
        print(f"Missing: {r['clean']}")
        invalid += 1

if invalid > 0:
    raise SystemExit(f"Found {invalid} invalid file references!")

print("Validation passed!")
```

---

### é—®é¢˜ 6ï¼šåˆ†ç‰‡å¯èƒ½å¯¼è‡´ç»“æœä¸ä¸€è‡´

**ä½ç½®**ï¼š`data/prepare_dataset.py` Line 395-401

**é—®é¢˜ä»£ç **ï¼š
```python
if args.shard_count > 1:
    shard_files = []
    for i, p in enumerate(clean_files):
        if i % args.shard_count == args.shard_idx:  # âŒ ç®€å•çš„æ¨¡è¿ç®—
            shard_files.append(p)
    clean_files = shard_files
```

**é—®é¢˜åˆ†æ**ï¼š
- åˆ†ç‰‡åœ¨éšæœºæŠ½å–å’Œæ‰“ä¹±ä¹‹åè¿›è¡Œ
- å¦‚æœ clean_files é¡ºåºä¸ä¸€è‡´ï¼ˆä¸åŒè¿è¡Œï¼‰ï¼Œåˆ†ç‰‡ç»“æœä¼šä¸åŒ
- **å¯¼è‡´ä¸åŒåˆ†ç‰‡è¿è¡Œå¯èƒ½å¤„ç†ç›¸åŒæ–‡ä»¶æˆ–æ¼æ‰æ–‡ä»¶**

**å½±å“**ï¼šå¤šè¿›ç¨‹å¹¶è¡Œæ—¶å¯èƒ½äº§ç”Ÿé‡å¤æˆ–é—æ¼

**ä¿®å¤**ï¼š
```python
# åœ¨åˆ†ç‰‡å‰ç¡®ä¿é¡ºåºä¸€è‡´
clean_files = sorted(clean_files)  # æŒ‰è·¯å¾„æ’åº

if args.shard_count > 1:
    shard_files = []
    for i, p in enumerate(clean_files):
        if i % args.shard_count == args.shard_idx:
            shard_files.append(p)
    clean_files = shard_files
    print(f"Shard {args.shard_idx}/{args.shard_count} -> {len(clean_files)} files")
```

---

## ğŸŸ¡ ä¸­ç­‰é—®é¢˜ï¼ˆå»ºè®®ä¿®å¤ï¼‰

### é—®é¢˜ 7ï¼šè®­ç»ƒå¾ªç¯ä¸­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°åœ¨å¾ªç¯å†…

**ä½ç½®**ï¼š`train.py` Line 587-591

**å½“å‰ä»£ç **ï¼š
```python
# Line 562-591
for batch_idx, (degraded, clean) in enumerate(pbar):
    losses = self.train_step(degraded, clean)
    self.global_step += 1
    # ...
    
    # CosineAnnealingWarmRestarts æŒ‰ step æ›´æ–°
    if not self.scheduler_step_per_epoch and num_batches > 0:
        if self.enable_scheduler:
            step_frac = epoch + batch_idx / num_batches
            self.scheduler_g.step(step_frac)
            self.scheduler_d.step(step_frac)
```

**é—®é¢˜**ï¼šå·²ç»åœ¨å¾ªç¯å†…ï¼Œä½†é€»è¾‘æ­£ç¡®ã€‚âœ… æ— é—®é¢˜

**ï¼ˆæ’¤å›æ­¤é—®é¢˜ï¼Œä»£ç å·²ä¿®å¤ï¼‰**

---

### é—®é¢˜ 8ï¼šéªŒè¯é›†å¯èƒ½è¿‡å°

**ä½ç½®**ï¼š`configs/default.yaml` Line 39

**é—®é¢˜**ï¼š
```yaml
val_ratio: 0.05  # åªæœ‰ 5%
```

**åˆ†æ**ï¼š
- å¦‚æœæ€»æ ·æœ¬é‡å°ï¼ˆå¦‚ 1000ï¼‰ï¼ŒéªŒè¯é›†åªæœ‰ 50 ä¸ª
- éªŒè¯æŒ‡æ ‡å¯èƒ½ä¸ç¨³å®š

**å»ºè®®**ï¼šå¦‚æœæ€»æ ·æœ¬é‡ < 2000ï¼Œå»ºè®®å¢åŠ åˆ° 0.1ï¼ˆ10%ï¼‰

---

### é—®é¢˜ 9ï¼šnum_workers å¯èƒ½è¿‡å°

**ä½ç½®**ï¼š`configs/default.yaml` Line 47

**é—®é¢˜**ï¼š
```yaml
num_workers: 2  # å¯èƒ½å¤ªä¿å®ˆ
```

**åˆ†æ**ï¼š
- 2 ä¸ª worker å¯èƒ½ä¸è¶³ä»¥å–‚é¥± GPU
- ç‰¹åˆ«æ˜¯å¯ç”¨äº† align_df_delay æ—¶ï¼Œæ•°æ®åŠ è½½å¼€é”€å¤§

**å»ºè®®**ï¼š
- å•å¡è®­ç»ƒï¼š4-6 workers
- å¤šå¡è®­ç»ƒï¼šæ¯å¡ 2-4 workers
- æ ¹æ® CPU æ ¸å¿ƒæ•°å’Œ GPU æ•°é‡è°ƒæ•´

---

### é—®é¢˜ 10ï¼šæŒä¹…åŒ– worker æœªå¯ç”¨

**ä½ç½®**ï¼š`train.py` Line 284, 296

**é—®é¢˜**ï¼š
```python
persistent_workers=False,  # âŒ æ¯ä¸ª epoch éƒ½é‡å¯ worker
```

**åˆ†æ**ï¼š
- num_workers > 0 æ—¶ï¼Œpersistent_workers=False ä¼šå¯¼è‡´æ¯ä¸ª epoch ç»“æŸå worker è¿›ç¨‹è¢«é”€æ¯ï¼Œä¸‹ä¸ª epoch é‡æ–°å¯åŠ¨
- å¢åŠ å¼€é”€ï¼Œæµªè´¹æ—¶é—´

**ä¿®å¤**ï¼š
```python
persistent_workers=True if data_config['num_workers'] > 0 else False,
```

---

### é—®é¢˜ 11ï¼šprefetch_factor è¿‡å°

**ä½ç½®**ï¼š`train.py` Line 285, 297

**é—®é¢˜**ï¼š
```python
prefetch_factor=1,  # âŒ åªé¢„å– 1 ä¸ª batch
```

**å»ºè®®**ï¼š
```python
prefetch_factor=2 if data_config['num_workers'] > 0 else None,
```

---

## ğŸŸ¢ è½»å¾®é—®é¢˜

### é—®é¢˜ 12ï¼šç¼ºå°‘è®­ç»ƒæ•°æ®ç»Ÿè®¡

**å»ºè®®**ï¼šåœ¨è®­ç»ƒå¼€å§‹å‰æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯
```python
# åœ¨ train.py å¼€å§‹è®­ç»ƒå‰
if self.is_main:
    print(f"\nDataset Statistics:")
    print(f"  Train batches: {len(self.train_loader)}")
    print(f"  Val batches: {len(self.val_loader)}")
    print(f"  Samples per epoch: {len(self.train_loader) * train_config['batch_size']}")
```

---

### é—®é¢˜ 13ï¼šDeepFilterNet æ‰¹å¤„ç†æœªä¼˜åŒ–

**ä½ç½®**ï¼š`data/prepare_dataset.py` Line 287-304

**é—®é¢˜**ï¼šé€ä¸ªæ–‡ä»¶å¤„ç† DFï¼Œæœªåˆ©ç”¨æ‰¹å¤„ç†

**å½±å“**ï¼šæ•°æ®å‡†å¤‡é˜¶æ®µè¾ƒæ…¢

---

## ğŸ“‹ ä¿®å¤ä¼˜å…ˆçº§

### ç«‹å³ä¿®å¤ï¼ˆ5-10åˆ†é’Ÿï¼‰

1. **é—®é¢˜ 3**ï¼šskip_existing æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥ï¼ˆ5åˆ†é’Ÿï¼‰
2. **é—®é¢˜ 4**ï¼šæ•°æ®å‡†å¤‡ç»“æœè¿‡æ»¤ä¼˜åŒ–ï¼ˆ3åˆ†é’Ÿï¼‰
3. **é—®é¢˜ 5**ï¼šæ·»åŠ æœ€ç»ˆéªŒè¯ï¼ˆ3åˆ†é’Ÿï¼‰
4. **é—®é¢˜ 6**ï¼šåˆ†ç‰‡å‰æ’åºï¼ˆ2åˆ†é’Ÿï¼‰

### å»ºè®®ä¿®å¤ï¼ˆ10åˆ†é’Ÿï¼‰

5. **é—®é¢˜ 1-2**ï¼šç»Ÿä¸€é…ç½®å’Œä»£ç é»˜è®¤å€¼ï¼ˆ5åˆ†é’Ÿï¼‰
6. **é—®é¢˜ 10-11**ï¼šä¼˜åŒ– DataLoader å‚æ•°ï¼ˆ5åˆ†é’Ÿï¼‰

### å¯é€‰ä¼˜åŒ–

7. **é—®é¢˜ 8-9**ï¼šè°ƒæ•´é…ç½®å‚æ•°ï¼ˆæŒ‰éœ€ï¼‰
8. **é—®é¢˜ 12-13**ï¼šæ·»åŠ ç»Ÿè®¡å’Œä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

---

## âš ï¸ æ•°æ®å‡†å¤‡å‰æ£€æŸ¥æ¸…å•

åœ¨è¿è¡Œ `prepare_dataset.py` ä¹‹å‰ï¼š

- [ ] ç¡®è®¤æ•°æ®è·¯å¾„å­˜åœ¨ä¸”å¯è®¿é—®
- [ ] ç¡®è®¤æœ‰è¶³å¤Ÿç£ç›˜ç©ºé—´ï¼ˆè‡³å°‘ 3x åŸå§‹æ•°æ®å¤§å°ï¼‰
- [ ] æµ‹è¯• DeepFilterNet æ˜¯å¦æ­£å¸¸å·¥ä½œ
- [ ] å°è§„æ¨¡æµ‹è¯•ï¼ˆ--max_files 10ï¼‰

**æµ‹è¯•å‘½ä»¤**ï¼š
```bash
# å°è§„æ¨¡æµ‹è¯•
python data/prepare_dataset.py \
  --config configs/default.yaml \
  --max_files 10 \
  --skip_existing

# æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
ls -lh /data/train_data_lite/clean/ | head
ls -lh /data/train_data_lite/degraded/ | head

# éªŒè¯éŸ³é¢‘æ–‡ä»¶å¯è¯»
python -c "import soundfile as sf; sf.read('/data/train_data_lite/clean/00000000_00.wav')"
```

---

## âš ï¸ è®­ç»ƒå‰æ£€æŸ¥æ¸…å•

åœ¨è¿è¡Œ `train.py` ä¹‹å‰ï¼š

- [ ] ç¡®è®¤ train.txt å’Œ val.txt å­˜åœ¨
- [ ] ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼ˆè¿è¡Œ dataset.py æµ‹è¯•ï¼‰
- [ ] ç¡®è®¤ GPU å†…å­˜è¶³å¤Ÿï¼ˆbatch_size=16 éœ€è¦çº¦ 10-12GBï¼‰
- [ ] ç¡®è®¤æ—¥å¿—ç›®å½•å¯å†™

**æµ‹è¯•å‘½ä»¤**ï¼š
```bash
# æµ‹è¯•æ•°æ®åŠ è½½
python data/dataset.py /data/train_data_lite/train.txt

# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
python -c "
import torch
from model.generator import CausalUNetGenerator
model = CausalUNetGenerator()
x = torch.randn(2, 1, 48000)
y = model(x)
print(f'Input: {x.shape}, Output: {y.shape}')
print(f'Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M')
"
```

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

### ç«‹å³æ‰§è¡Œï¼ˆå¿…é¡»ï¼‰

1. **ä¿®å¤é—®é¢˜ 3-6**ï¼ˆ13åˆ†é’Ÿï¼‰
2. **å°è§„æ¨¡æ•°æ®å‡†å¤‡æµ‹è¯•**ï¼ˆ10åˆ†é’Ÿï¼‰
3. **ä¿®å¤é—®é¢˜ 1-2ã€10-11**ï¼ˆ10åˆ†é’Ÿï¼‰
4. **å°è§„æ¨¡è®­ç»ƒæµ‹è¯•**ï¼ˆ30åˆ†é’Ÿï¼Œ5 epochsï¼‰

### å…¨é‡è¿è¡Œï¼ˆç¡®è®¤æ— è¯¯åï¼‰

```bash
# æ•°æ®å‡†å¤‡ï¼ˆæ¨èåˆ†ç‰‡å¹¶è¡Œï¼‰
for i in 0 1 2 3; do
  CUDA_VISIBLE_DEVICES=$i \
  python data/prepare_dataset.py \
    --config configs/default.yaml \
    --shard-idx $i --shard-count 4 \
    --skip_existing \
    --num_workers 12 &
done
wait

# éªŒè¯æ•°æ®å®Œæ•´æ€§
python -c "
with open('/data/train_data_lite/train.txt') as f:
    lines = f.readlines()
print(f'Train samples: {len(lines)}')
with open('/data/train_data_lite/val.txt') as f:
    lines = f.readlines()
print(f'Val samples: {len(lines)}')
"

# è®­ç»ƒ
torchrun --nproc_per_node=4 train.py --config configs/default.yaml
```

---

## âš¡ é¢„æœŸæ€§èƒ½

ä¿®å¤åçš„é¢„æœŸæ€§èƒ½ï¼š

**æ•°æ®å‡†å¤‡**ï¼š
- å•è¿›ç¨‹ï¼š~100-200 samples/min
- 4è¿›ç¨‹åˆ†ç‰‡ï¼š~400-800 samples/min

**è®­ç»ƒ**ï¼š
- å•å¡ 4090ï¼š~2-3 batches/sec (batch_size=16)
- 4å¡ 4090ï¼š~8-12 batches/sec

**GPU åˆ©ç”¨ç‡**ï¼š
- ä¿®å¤å‰ï¼š30-50%ï¼ˆæ•°æ®åŠ è½½ç“¶é¢ˆï¼‰
- ä¿®å¤åï¼š80-95%

---

**å®¡è®¡å®Œæˆ**

å…±å‘ç°ï¼š
- ğŸ”´ ä¸¥é‡é—®é¢˜ï¼š6ä¸ªï¼ˆå¿…é¡»ä¿®å¤ï¼‰
- ğŸŸ¡ ä¸­ç­‰é—®é¢˜ï¼š5ä¸ªï¼ˆå»ºè®®ä¿®å¤ï¼‰
- ğŸŸ¢ è½»å¾®é—®é¢˜ï¼š2ä¸ªï¼ˆå¯é€‰ï¼‰

**é¢„è®¡ä¿®å¤æ—¶é—´**ï¼š30-40åˆ†é’Ÿ

**å»ºè®®**ï¼šå…ˆä¿®å¤é—®é¢˜ 1-6 å’Œ 10-11ï¼Œç„¶åå°è§„æ¨¡æµ‹è¯•ï¼Œç¡®è®¤æ— è¯¯åå…¨é‡è¿è¡Œã€‚

